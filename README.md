Жизнь может вывести нас из равновесия различными испытаниями и эмоциями. Иногда поделиться этими чувствами с близкими нелегко, и некоторые аспекты мы предпочитаем оставить в секрете. 
Этот проект направлен на предоставление решения – компаньона на базе искусственного интеллекта в виде Telegram-бота.

[<i class="fas fa-file-alt"></i>Google Colab с кодом и более подробным описанием](https://colab.research.google.com/drive/1ZQBwU21uCrrjsNR5X2uJwsC6JihxACij)

## Основные характеристики
- **Распознавание эмоций:** Бот анализирует сообщения пользователей и идентифицирует лежащие в их основе эмоции. 
- **Персонализированные рекомендации:** Основываясь на распознанных эмоциях, бот предлагает индивидуальные рекомендации, которые могут помочь в управлении эмоциональным состоянием.
- **Еженедельные сводки:** Бот возвращает краткое содержание сообщений, позволяя рефлексировать о пережитом опыте.

## Под капотом
### Модель классификации
- Был собран и вручную размечен набор данных (3010 записей ☹)
![[images/data.png]]
- Модель классификации основана на модели `cointegrated/rubert-tiny2`, уменьшенной BERT-модели для работы только с русским языком, к которой был добавлен верхний слой для задачи многометочной классификации
- Использовался токенизатор, соответствующий данной модели
- Для дообучения модели использовалась 10-блочная кросс-валидация
- На каждом блоке модель обучалась в течение 5 эпох
- В качестве метрики оценки качества модели на валидационных данных использовалась `F1-score`, так как набор данных получился несбалансированный и хотелось уменьшить как количество ложноположительных, так и ложноотрицательных предсказаний
- Лучший `F1-score` на валидационной выборке: 91.75%.

Модель `cointegrated/rubert-tiny2` была выбрана из-за легковесности и того, что она демонстрирует неплохие результаты.

### Модель суммаризации
- Использовалась мультиязычная модель генерации текста T5 (причиной ее выбора являлось *наличие многоязычной уменьшенной модели*)
- Она была сжата для работы только с русским языком (1.2 Гбайт → 230 Мбайт, 300 млн параметров → 60 млн):
	- Извлечены 15000 наиболее популярных русских токенов
	- Параметры входных и выходных эмбеддингов были заменены на эмбеддинги полученных токенов
	- Оригинальные токены токенизатора заменены на полученные
- Сбор данных для файнтьюнинга модели под задачу проводился вручную в многопоточном режиме с помощью парсинга страниц woman.ru для извлечения текста записей с форума и их заголовков (всего 20000 записей)
- Записи были обработаны (осталось 14126 записей)
- Сжатая модель обучалась в течение 4 эпох

## Принцип работы
![[images/scheme.png]]
- Пользователи отправляют боту сообщения (например, что с ними происходило в течение дня)
- Тексты сообщений сохраняются и отправляются в классификатор
- Классификатор делит текст на предложения, обрабатывает их с использованием модели классификации и объединяет метки эмоций для определения общего эмоционального контекста
- Найденные эмоции передаются в модуль рекомендаций для получения советов, соответствующих эмоциональному состоянию отправителя
- Найденные эмоции и рекомендации возвращаются пользователю
- Раз в неделю с помощью сообщения пользователей обрабатываются генератором обобщений и возвращаются им в сжатом виде, после чего удаляются, что частично обеспечивает конфиденциальность персональных данных. 
	- Краткое содержание отправляется для того, чтобы люди проанализировали произошедшее.
    ![[images/processing.png]]
## Примеры работы бота
![[images/examples.png]]
## В планах
- Усовершенствовать модель распознавания эмоций путем сбора большего количества данных и увеличение спектра эмоций для повышения точности модели классификации эмоций и выдачи более персонализированных рекомендаций.
